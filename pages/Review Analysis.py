import streamlit as st
import streamlit.components.v1 as components
from streamlit_tags import st_tags

import plotly.express as px
import plotly.graph_objects as go

# ê¸°ë³¸ ë¼ì´ë¸ŒëŸ¬ë¦¬
import os
import ast
from datetime import datetime
from datetime import timedelta

import warnings
warnings.filterwarnings("ignore", message="PyplotGlobalUseWarning")

from collections import Counter
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import koreanize_matplotlib

from PIL import Image

from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer

from gensim.models import Word2Vec
import networkx as nx
import gensim
from pyvis.network import Network
from wordcloud import WordCloud
########################################################################################################################
# ë°ì´í„° ë¡œë“œ ìƒìˆ˜
df_ë¦¬ë·°_ê°ì„±ë¶„ì„ê²°ê³¼ = pd.read_csv('/app/busypeople-streamlit/data/ë¦¬ë·°7ì°¨(ìˆ˜ì •).csv')
df_ë¦¬ë·°_ê°ì„±ë¶„ì„ê²°ê³¼['time'] = pd.to_datetime(df_ë¦¬ë·°_ê°ì„±ë¶„ì„ê²°ê³¼['time'])

# df_ë¦¬ë·°_ê°ì„±ë¶„ì„ê²°ê³¼['time'] = pd.to_datetime(df_ë¦¬ë·°_ê°ì„±ë¶„ì„ê²°ê³¼['time'], format='%Y-%m-%d')


stopwords = ['ì–¸ëŠ˜', 'ê²°êµ­', 'ìƒê°', 'í›„ê¸°', 'ê°ì‚¬', 'ì§„ì§œ', 'ì™„ì „', 'ì‚¬ìš©', 'ìš”ì¦˜', 'ì •ë„', 'ì´ë²ˆ', 'ë‹¬ë¦¬ë·°', 'ê²°ê³¼', 
             'ì§€ê¸ˆ', 'ë™ì˜ìƒ', 'ì¡°ê¸ˆ', 'ì•ˆí…Œ', 'ì…ì œ', 'ì˜ìƒ', 'ì´ë²ˆê±´', 'ë©°ì¹ ', 'ì´ì œ', 'ê±°ì‹œê¸°', 'ì–¼ë“¯', 'ì²˜ìŒ', 'ë‹¤ìŒ',
             'í•©ë‹ˆë‹¤', 'í•˜ëŠ”', 'í• ', 'í•˜ê³ ', 'í•œë‹¤','í•˜ë‹¤','ë˜ë‹¤','ê°™ë‹¤','ìë‹¤','ë˜ë‹¤','ìˆë‹¤','ì¨ë‹¤','ì•Šë‹¤','í•´ë³´ë‹¤','ì£¼ë‹¤','ë˜ì–´ë‹¤', 
             'ê·¸ë¦¬ê³ ', 'ì…ë‹ˆë‹¤', 'ê·¸', 'ë“±', 'ì´ëŸ°', 'ë°','ì œ', 'ë”','ì–¸ëŠ˜','ê²°êµ­','ìƒê°','ì‹ë¬¼í‚¤',
             'ê°ì‚¬','ì§„ì§œ','ì™„ì „','ìš”ã…','ì‚¬ìš©','ì •ë„','ì—„ë§ˆ','ì•„ì´','ì›ë˜','íí','í•˜í•˜','ì •ë§']

########################################################################################################################
# title
st.title('ìì‚¬/ê²½ìŸì‚¬ ë¦¬ë·° ëª¨ë‹ˆí„°ë§ ëŒ€ì‹œë³´ë“œ')      
st.write("")
st.write("")
st.write("")
########################################################################################################################
# ë ˆì´ì•„ì›ƒ
#1_1 : í’ˆì‚¬, 1_2 : ì œí’ˆ, 1_3 : ì‹œì‘ë‚ ì§œ, 1_4: ë§ˆì§€ë§‰ ë‚ ì§œ
#2_1 : ì›Œë“œ í´ë¼ìš°ë“œ ì„¸ë¶€ í•„í„°
#3_1,2 : ê¸°ì¤€, 3_3,4 : ë‹¨ì–´ ìˆ˜ ì¡°ì •
#4_1,2,3,4 : í¬í•¨X  ë‹¨ì–´
########################################################################################################################
# ì›Œí´, ë„½ì›¤ ê³µí†µí•„í„° ë ˆì´ì•„ì›ƒ
# 0. ê¸/ë¶€ì •
with st.container():
    col0_1, col0_2, col0_3 = st.columns([1,1,1])
# 1. ì›Œí´, ë„½ì›¤ ê³µí†µ ì˜µì…˜
with st.container():
    col1_0, col1_1, col1_2, col1_3, col1_4 = st.columns([1,1,1,1,1])
########################################################################################################################
# ì›Œí´, ë„½ì›¤ ê³µí†µí•„í„°


with col0_1:
    st.markdown('ğŸšï¸ê¸°ë³¸ ì„¤ì •')

with col1_0:
    íšŒì‚¬ì¢…ë¥˜ = st.selectbox(
        'ì œí’ˆ',
        ('ìì‚¬+ê²½ìŸì‚¬', 'ê½ƒí”¼ìš°ëŠ” ì‹œê°„', 'ê²½ìŸì‚¬-ì‹ë¬¼ì˜ì–‘ì œ', 
         'ê²½ìŸì‚¬-ë¿Œë¦¬ì˜ì–‘ì œ', 
         'ê²½ìŸì‚¬-ì‚´ì¶©ì œ',
         'ê²½ìŸì‚¬-ì‹ë¬¼ë“±',
         'ê²½ìŸì‚¬ ì „ì²´',
         ))
    # st.write('ì„ íƒì œí’ˆ: ', íšŒì‚¬ì¢…ë¥˜)
    if íšŒì‚¬ì¢…ë¥˜ == 'ìì‚¬+ê²½ìŸì‚¬':
        íšŒì‚¬ì¢…ë¥˜ë§ˆìŠ¤í¬ = ((df_ë¦¬ë·°_ê°ì„±ë¶„ì„ê²°ê³¼['name'] == 'ê²½ìŸì‚¬') | (df_ë¦¬ë·°_ê°ì„±ë¶„ì„ê²°ê³¼['name'] == 'ê½ƒí”¼ìš°ëŠ”ì‹œê°„'))
    if íšŒì‚¬ì¢…ë¥˜ == 'ê½ƒí”¼ìš°ëŠ” ì‹œê°„':
        íšŒì‚¬ì¢…ë¥˜ë§ˆìŠ¤í¬ = (df_ë¦¬ë·°_ê°ì„±ë¶„ì„ê²°ê³¼['name'] == 'ê½ƒí”¼ìš°ëŠ”ì‹œê°„')
    if íšŒì‚¬ì¢…ë¥˜ == 'ê²½ìŸì‚¬-ì‹ë¬¼ì˜ì–‘ì œ':
        íšŒì‚¬ì¢…ë¥˜ë§ˆìŠ¤í¬ = ((df_ë¦¬ë·°_ê°ì„±ë¶„ì„ê²°ê³¼['name'] == 'ê²½ìŸì‚¬') & (df_ë¦¬ë·°_ê°ì„±ë¶„ì„ê²°ê³¼['item'] == 'ì‹ë¬¼ì˜ì–‘ì œ'))
    if íšŒì‚¬ì¢…ë¥˜ == 'ê²½ìŸì‚¬-ë¿Œë¦¬ì˜ì–‘ì œ':
        íšŒì‚¬ì¢…ë¥˜ë§ˆìŠ¤í¬ = ((df_ë¦¬ë·°_ê°ì„±ë¶„ì„ê²°ê³¼['name'] == 'ê²½ìŸì‚¬') & (df_ë¦¬ë·°_ê°ì„±ë¶„ì„ê²°ê³¼['item'] == 'ë¿Œë¦¬ì˜ì–‘ì œ'))
    if íšŒì‚¬ì¢…ë¥˜ == 'ê²½ìŸì‚¬-ì‚´ì¶©ì œ':
        íšŒì‚¬ì¢…ë¥˜ë§ˆìŠ¤í¬ = ((df_ë¦¬ë·°_ê°ì„±ë¶„ì„ê²°ê³¼['name'] == 'ê²½ìŸì‚¬') & (df_ë¦¬ë·°_ê°ì„±ë¶„ì„ê²°ê³¼['item'] == 'ì‚´ì¶©ì œ'))
    if íšŒì‚¬ì¢…ë¥˜ == 'ê²½ìŸì‚¬-ì‹ë¬¼ë“±':
        íšŒì‚¬ì¢…ë¥˜ë§ˆìŠ¤í¬ = ((df_ë¦¬ë·°_ê°ì„±ë¶„ì„ê²°ê³¼['name'] == 'ê²½ìŸì‚¬') & (df_ë¦¬ë·°_ê°ì„±ë¶„ì„ê²°ê³¼['item'] == 'ì‹ë¬¼ë“±'))
    if íšŒì‚¬ì¢…ë¥˜ == 'ê²½ìŸì‚¬ ì „ì²´':
        íšŒì‚¬ì¢…ë¥˜ë§ˆìŠ¤í¬ = (df_ë¦¬ë·°_ê°ì„±ë¶„ì„ê²°ê³¼['name'] == 'ê²½ìŸì‚¬')

with col1_1:
    # st.secrets['API_KEY']
    ê¸ë¶€ì • = st.selectbox(
    "ë¦¬ë·° ìœ í˜•", ('ì „ì²´', 'ê¸ì •', 'ë¶€ì •'))
if ê¸ë¶€ì • == 'ì „ì²´':
    ê¸ë¶€ì •ë§ˆìŠ¤í¬ = ((df_ë¦¬ë·°_ê°ì„±ë¶„ì„ê²°ê³¼['sentiment'] == 'ê¸ì •') | (df_ë¦¬ë·°_ê°ì„±ë¶„ì„ê²°ê³¼['sentiment'] == 'ë¶€ì •'))
if ê¸ë¶€ì • == 'ê¸ì •':
    ê¸ë¶€ì •ë§ˆìŠ¤í¬ = (df_ë¦¬ë·°_ê°ì„±ë¶„ì„ê²°ê³¼['sentiment'] == 'ê¸ì •')
if ê¸ë¶€ì • == 'ë¶€ì •':
    ê¸ë¶€ì •ë§ˆìŠ¤í¬ = (df_ë¦¬ë·°_ê°ì„±ë¶„ì„ê²°ê³¼['sentiment'] == 'ë¶€ì •')

with col1_2:
    í’ˆì‚¬ì˜µì…˜ = st.selectbox(
        'í‚¤ì›Œë“œ ìœ í˜•',
        ('ëª…ì‚¬', 'ëª…ì‚¬+ë™ì‚¬+í˜•ìš©ì‚¬'))
    # st.write('ì„ íƒí’ˆì‚¬: ', í’ˆì‚¬ì˜µì…˜)


ì‹œì‘ë‚ ì§œ = df_ë¦¬ë·°_ê°ì„±ë¶„ì„ê²°ê³¼['time'][íšŒì‚¬ì¢…ë¥˜ë§ˆìŠ¤í¬].min()
ë§ˆì§€ë§‰ë‚ ì§œ = df_ë¦¬ë·°_ê°ì„±ë¶„ì„ê²°ê³¼['time'][íšŒì‚¬ì¢…ë¥˜ë§ˆìŠ¤í¬].max()

with col1_3:
    start_date = st.date_input(
        'ì‹œì‘ ë‚ ì§œ',
        value=ì‹œì‘ë‚ ì§œ,
        min_value=ì‹œì‘ë‚ ì§œ,
        max_value=ë§ˆì§€ë§‰ë‚ ì§œ
    )
with col1_4:
    end_date = st.date_input(
        'ë ë‚ ì§œ',
        value=ë§ˆì§€ë§‰ë‚ ì§œ,
        min_value=ì‹œì‘ë‚ ì§œ,
        max_value=ë§ˆì§€ë§‰ë‚ ì§œ
    )

########################################################################################################################
# ì›Œí´ ì„¸ë¶€ í•„í„°
# # 2,3. ì›Œí´ ì„¸ë¶€ í•„í„°
# with st.container():
#     col2_1, col2_2= st.columns([1,1])
# # 3. ì›Œí´ ì„¸ë¶€ í•„í„°
# with st.container():
#     col3_1, col3_2= st.columns([1,1])
st.write("")
st.write("")
st.write("")

st.subheader('**ğŸ” ì¤‘ìš” í‚¤ì›Œë“œ ë°œêµ´**')
expander = st.expander('ì„¸ë¶€í•„í„°')
with expander:
    col2_1, col2_2= st.columns(2)    
    with col2_1:
        option = st.selectbox(
            'ê¸°ì¤€',
            ('ë¹ˆë„(Count)', 'ìƒëŒ€ ë¹ˆë„(TF-IDF)'), help='**ë„ì›€ë§**\n\n'
                    'Count: ë‹¨ì–´ì˜ ë¹ˆë„ ìˆœìœ¼ë¡œ í¬ê¸°ë¥¼ ì„¤ì •í•©ë‹ˆë‹¤.\n\n'
                    'TF-IDF: ì „ì²´ ë¦¬ë·° ë‚´ ë¹ˆë„ì™€ ê°œë³„ ë¦¬ë·° ë‚´ ë¹ˆë„ë¥¼ ëª¨ë‘ ê³ ë ¤í•´ í¬ê¸°ë¥¼ ì„¤ì •í•©ë‹ˆë‹¤.')
        # st.write('ì„ íƒê¸°ì¤€: ', option)

    with col2_2:
        ë‹¨ì–´ìˆ˜ = st.slider(
            'í‚¤ì›Œë“œ ìˆ˜',
            10, 300, step=1)
        # st.write('ë‹¨ì–´ìˆ˜: ', ë‹¨ì–´ìˆ˜)
   
    stopwords = st_tags(
        label = 'ì œê±°í•  í‚¤ì›Œë“œ',
        text = 'ì§ì ‘ ì…ë ¥í•´ë³´ì„¸ìš”',
        value = ['ì‹ë¬¼', 'íš¨ê³¼', 'ë°°ì†¡'],
        suggestions = ['ì‹ë¬¼', 'íš¨ê³¼', 'ë°°ì†¡'],
        key = '1')

# 4. ì›Œí´ + ë°”ì°¨íŠ¸
with st.container():
    col4_1, col4_2 = st.columns([2,2])

# with col2_1:
#     option = st.selectbox(
#         'ğŸ€ë‹¨ì–´ê¸°ì¤€ì„ íƒğŸ€',
#         ('ë‹¨ìˆœ ë¹ˆë„(Countvecterize)', 'ìƒëŒ€ ë¹ˆë„(TF-IDF)'))
#     st.write('ì„ íƒê¸°ì¤€: ', option)

# with col2_2:
#     ë‹¨ì–´ìˆ˜ = st.slider(
#         'ğŸ€ë‹¨ì–´ ìˆ˜ ì¡°ì •í•˜ê¸°ğŸ€',
#         10, 300, step=1)
#     st.write('ë‹¨ì–´ìˆ˜: ', ë‹¨ì–´ìˆ˜)

# with col3_1:
#     ì¶”ê°€ë¶ˆìš©ì–´ = st.text_input('ğŸ€í¬í•¨í•˜ì§€ ì•Šì„ ë‹¨ì–´ì…ë ¥ğŸ€', '')
#     if ì¶”ê°€ë¶ˆìš©ì–´ == '':
#         st.write('ì˜ˆì‹œ : ì˜ì–‘ì œ, ì‹ë¬¼, ë°°ì†¡')
#     if ì¶”ê°€ë¶ˆìš©ì–´ != '':
#         st.write('ì œê±°í•œ ë‹¨ì–´: ', ì¶”ê°€ë¶ˆìš©ì–´)

########################################################################################################################
# ì›Œë“œ í´ë¼ìš°ë“œ 
def get_count_top_words(df, start_date=None, last_date=None, num_words=200, name=None, sentiment = None, item = None, source = None , í’ˆì‚¬='noun'):
    if name is not None:
        df = df[df['name'] == name]
    if sentiment is not None:
        df = df[df['sentiment'] == sentiment]
    if item is not None:
        df = df[df['item'] == item]
    if source is not None:
        df = df[df['source'] == source]
    if start_date is None:
        start_date = df['time'].min().strftime('%Y-%m-%d')
    if last_date is None:
        last_date = df['time'].max().strftime('%Y-%m-%d')
    df = df[(df['time'] >= start_date) & (df['time'] <= last_date)]
    count_vectorizer = CountVectorizer(stop_words=stopwords)
    count = count_vectorizer.fit_transform(df[í’ˆì‚¬].values)
    count_df = pd.DataFrame(count.todense(), columns=count_vectorizer.get_feature_names_out())
    count_top_words = count_df.sum().sort_values(ascending=False).head(num_words).to_dict()
    return count_top_words

def get_tfidf_top_words(df, start_date=None, last_date=None, num_words=200, name=None, sentiment = None, item = None, source = None, í’ˆì‚¬='noun' ):
    if name is not None:
        df = df[df['name'] == name]
    if sentiment is not None:
        df = df[df['sentiment'] == sentiment]
    if item is not None:
        df = df[df['item'] == item]
    if source is not None:
        df = df[df['source'] == source]
    if start_date is None:
        start_date = df['time'].min().strftime('%Y-%m-%d')
    if last_date is None:
        last_date = df['time'].max().strftime('%Y-%m-%d')
    df = df[(df['time'] >= start_date) & (df['time'] <= last_date)]
    tfidf_vectorizer = TfidfVectorizer(stop_words=stopwords)
    tfidf = tfidf_vectorizer.fit_transform(df[í’ˆì‚¬].values)
    tfidf_df = pd.DataFrame(tfidf.todense(), columns=tfidf_vectorizer.get_feature_names_out())
    tfidf_top_words = tfidf_df.sum().sort_values(ascending=False).head(num_words).to_dict()
    return tfidf_top_words
########################################################################################################################


ê¸°ê°„ë§ˆìŠ¤í¬ = ((df_ë¦¬ë·°_ê°ì„±ë¶„ì„ê²°ê³¼['time'] >= pd.to_datetime(start_date)) & (df_ë¦¬ë·°_ê°ì„±ë¶„ì„ê²°ê³¼['time'] <= pd.to_datetime(end_date)))


# if ì¶”ê°€ë¶ˆìš©ì–´.find(',') != -1:
#     stopwords.extend([i.strip() for i in ì¶”ê°€ë¶ˆìš©ì–´.split(',')])
# if ì¶”ê°€ë¶ˆìš©ì–´.find(',') == -1:
#     stopwords.append(ì¶”ê°€ë¶ˆìš©ì–´) 

if í’ˆì‚¬ì˜µì…˜ == 'ëª…ì‚¬':
    í’ˆì‚¬ = 'noun'
if í’ˆì‚¬ì˜µì…˜ == 'ëª…ì‚¬+ë™ì‚¬+í˜•ìš©ì‚¬':
    í’ˆì‚¬ = 'n_v_ad'

ë§ˆìŠ¤í¬ëœë°ì´í„°í”„ë ˆì„ = df_ë¦¬ë·°_ê°ì„±ë¶„ì„ê²°ê³¼[ê¸ë¶€ì •ë§ˆìŠ¤í¬ & ê¸°ê°„ë§ˆìŠ¤í¬ & íšŒì‚¬ì¢…ë¥˜ë§ˆìŠ¤í¬]
reviews = [eval(i) for i in ë§ˆìŠ¤í¬ëœë°ì´í„°í”„ë ˆì„[í’ˆì‚¬]]

ì¹´ìš´íŠ¸ = get_count_top_words(ë§ˆìŠ¤í¬ëœë°ì´í„°í”„ë ˆì„, num_words=ë‹¨ì–´ìˆ˜, í’ˆì‚¬=í’ˆì‚¬)
tdidf = get_tfidf_top_words(ë§ˆìŠ¤í¬ëœë°ì´í„°í”„ë ˆì„, num_words=ë‹¨ì–´ìˆ˜, í’ˆì‚¬=í’ˆì‚¬)

if option == 'ë¹ˆë„(Count)':
    words = ì¹´ìš´íŠ¸
if option == 'ìƒëŒ€ ë¹ˆë„(TF-IDF)':
    words = tdidf

########################################################################################################################
# ì›Œë“œí´ë¼ìš°ë“œ
with col4_1:
    cand_mask = np.array(Image.open('/app/busypeople-streamlit/data/circle.png'))
    ì›Œë“œí´ë¼ìš°ë“œ = WordCloud(
        background_color="white", 
        max_words=1000,
        font_path = "/app/busypeople-streamlit/font/NanumBarunGothic.ttf", 
        contour_width=3, 
        colormap='Spectral', 
        contour_color='white',
        # mask=cand_mask,
        width=800,
        height=600
        ).generate_from_frequencies(words)

    st.image(ì›Œë“œí´ë¼ìš°ë“œ.to_array(), use_column_width=True)

with col4_2:
    # st.plotly_chart(words)
    st.markdown('**í‚¤ì›Œë“œ ë¹ˆë„ìˆ˜**')
    ë°”ì°¨íŠ¸ = go.Figure([go.Bar(x=list(words.keys()),y=list(words.values()))])
    st.plotly_chart(ë°”ì°¨íŠ¸, use_container_width=True)
########################################################################################################################
st.write("")
st.write("")
st.write("")

st.subheader('**ğŸ”ì—°ê´€ í‚¤ì›Œë“œ íƒìƒ‰**')

expander = st.expander('ì„¸ë¶€í•„í„°')
with expander:
        í‚¤ì›Œë“œ = st.text_input('ê¶ê¸ˆí•œ í‚¤ì›Œë“œ', 'ì‹ë¬¼')
        if í‚¤ì›Œë“œ == '':
            í‚¤ì›Œë“œ = ['ì‹ë¬¼']
            st.write('í‚¤ì›Œë“œë¥¼ ì…ë ¥í•´ì£¼ì„¸ìš”.')
            st.write(' ì˜ˆì‹œ : ë¿Œë¦¬, ì œë¼ëŠ„, ì‹ë¬¼, ì‘ì• ')
            st.write('ì„¤ì •ëœ í‚¤ì›Œë“œ: ', í‚¤ì›Œë“œ[0])
        elif í‚¤ì›Œë“œ.find(',') == -1:
            st.write('ì˜ˆì‹œ : ë¿Œë¦¬, ì œë¼ëŠ„, ì‹ë¬¼, ì‘ì• ')
            í‚¤ì›Œë“œ = [í‚¤ì›Œë“œ]
        elif í‚¤ì›Œë“œ.find(',') != -1:
            st.write('ì„¤ì •ëœ í‚¤ì›Œë“œ: ', í‚¤ì›Œë“œ)
            í‚¤ì›Œë“œ = [i.strip() for i in í‚¤ì›Œë“œ.split(',')]
        else:
            st.error('This is an error', icon="ğŸš¨")
# try:
#     í‚¤ì›Œë“œ = í‚¤ì›Œë“œ(standard_df, new_df)
# except:
#     st.warning("âš ï¸ í•´ë‹¹ ê¸°ê°„ ë™ì•ˆ ì‹ ê·œ í‚¤ì›Œë“œê°€ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤")

   

# # 5. ë„½ì›¤ ì„¸ë¶€í•„í„°
# with st.container():
#     col5_1, col5_2 = st.columns([1,1])
########################################################################################################################
# with col5_1:
#     í‚¤ì›Œë“œ = st.text_input('ğŸ€ë„¤íŠ¸ì›Œí¬ ë‹¨ì–´ì…ë ¥ğŸ€', 'ì œë¼ëŠ„')
#     if í‚¤ì›Œë“œ == '':
#         í‚¤ì›Œë“œ = ['ì œë¼ëŠ„']
#         st.write('ë‹¨ì–´ë¥¼ ì…ë ¥í•´ì£¼ì„¸ìš”.')
#         st.write(' ì˜ˆì‹œ : ë¿Œë¦¬, ì œë¼ëŠ„, ì‹ë¬¼, ì‘ì• ')
#         st.write('ì„¤ì •ëœ ë‹¨ì–´: ', í‚¤ì›Œë“œ[0])
#     elif í‚¤ì›Œë“œ.find(',') == -1:
#         st.write('ì˜ˆì‹œ : ë¿Œë¦¬, ì œë¼ëŠ„, ì‹ë¬¼, ì‘ì• ')
#         í‚¤ì›Œë“œ = [í‚¤ì›Œë“œ]
#     elif í‚¤ì›Œë“œ.find(',') != -1:
#         st.write('ì„¤ì •ëœ ë‹¨ì–´: ', í‚¤ì›Œë“œ)
#         í‚¤ì›Œë“œ = [i.strip() for i in í‚¤ì›Œë“œ.split(',')]
#     else:
#         # st.write('ë¬¸ì œê°€ ìƒê²¼ì–´ìš”.')

########################################################################################################################
# ë„¤íŠ¸ì›Œí¬ ì°¨íŠ¸

def ë„¤íŠ¸ì›Œí¬(reviews):
    networks = []
    for review in reviews:
        network_review = [w for w in review if len(w) > 1]
        networks.append(network_review)

    model = Word2Vec(networks, vector_size=100, window=5, min_count=1, workers=4, epochs=100)

    G = nx.Graph(font_path='/app/busypeople-streamlit/font/NanumBarunGothic.ttf')

    # ì¤‘ì‹¬ ë…¸ë“œë“¤ì„ ë…¸ë“œë¡œ ì¶”ê°€
    for keyword in í‚¤ì›Œë“œ:
        G.add_node(keyword)
        # ì£¼ì–´ì§„ í‚¤ì›Œë“œì™€ ê°€ì¥ ìœ ì‚¬í•œ 20ê°œì˜ ë‹¨ì–´ ì¶”ì¶œ
        similar_words = model.wv.most_similar(keyword, topn=20)
        # ìœ ì‚¬í•œ ë‹¨ì–´ë“¤ì„ ë…¸ë“œë¡œ ì¶”ê°€í•˜ê³ , ì£¼ì–´ì§„ í‚¤ì›Œë“œì™€ì˜ ì—°ê²°ì„  ì¶”ê°€
        for word, score in similar_words:
            G.add_node(word)
            G.add_edge(keyword, word, weight=score)
            
    # ë…¸ë“œ í¬ê¸° ê²°ì •
    size_dict = nx.degree_centrality(G)

    # ë…¸ë“œ í¬ê¸° ì„¤ì •
    node_size = []
    for node in G.nodes():
        if node in í‚¤ì›Œë“œ:
            node_size.append(5000)
        else:
            node_size.append(1000)

    # í´ëŸ¬ìŠ¤í„°ë§
    clusters = list(nx.algorithms.community.greedy_modularity_communities(G))
    cluster_labels = {}
    for i, cluster in enumerate(clusters):
        for node in cluster:
            cluster_labels[node] = i
            
    # ë…¸ë“œ ìƒ‰ìƒ ê²°ì •
    color_palette = ["#f39c9c", "#f7b977", "#fff4c4", "#d8f4b9", "#9ed6b5", "#9ce8f4", "#a1a4f4", "#e4b8f9", "#f4a2e6", "#c2c2c2"]
    node_colors = [color_palette[cluster_labels[node] % len(color_palette)] for node in G.nodes()]

    # ë…¸ë“œì— ë¼ë²¨ê³¼ ì—°ê²° ê°•ë„ ê°’ ì¶”ê°€
    edge_weights = [d['weight'] for u, v, d in G.edges(data=True)]

    # ì„ ì˜ ê¸¸ì´ë¥¼ ë³€ê²½ pos
    # plt.figure(figsize=(15,15))
    pos = nx.spring_layout(G, seed=42, k=0.15)
    nx.draw(G, pos, font_family='NanumGothic', with_labels=True, node_size=node_size, node_color=node_colors, alpha=0.8, linewidths=1,
            font_size=9, font_color="black", font_weight="medium", edge_color="grey", width=edge_weights)


    # ì¤‘ì‹¬ ë…¸ë“œë“¤ë¼ë¦¬ ê²¹ì¹˜ëŠ” ë‹¨ì–´ ì¶œë ¥
    overlapping_í‚¤ì›Œë“œ = set()
    for i, keyword1 in enumerate(í‚¤ì›Œë“œ):
        for j, keyword2 in enumerate(í‚¤ì›Œë“œ):
            if i < j and keyword1 in G and keyword2 in G:
                if nx.has_path(G, keyword1, keyword2):
                    overlapping_í‚¤ì›Œë“œ.add(keyword1)
                    overlapping_í‚¤ì›Œë“œ.add(keyword2)
    if overlapping_í‚¤ì›Œë“œ:
        print(f"ë‹¤ìŒ ì¤‘ì‹¬ í‚¤ì›Œë“œë“¤ë¼ë¦¬ ì—°ê´€ì„±ì´ ìˆì–´ ì¤‘ë³µë  ê°€ëŠ¥ì„±ì´ ìˆìŠµë‹ˆë‹¤: {', '.join(overlapping_í‚¤ì›Œë“œ)}")


    net = Network(notebook=True, cdn_resources='in_line')

    net.from_nx(G)

    return [net, similar_words]

ë„¤íŠ¸ì›Œí¬ = ë„¤íŠ¸ì›Œí¬(reviews)
########################################################################################################################
# 6. ë„½ì›¤ + íŒŒì´ì°¨íŠ¸
with st.container():
    col6_1, col6_2 = st.columns([3,1])

with col6_1:
    try:
        net = ë„¤íŠ¸ì›Œí¬[0]
        net.save_graph(f'/app/busypeople-streamlit/pyvis_graph.html')
        HtmlFile = open(f'/app/busypeople-streamlit/pyvis_graph.html', 'r', encoding='utf-8')
        components.html(HtmlFile.read(), height=435)
    except:
        st.write('ì¡´ì¬í•˜ì§€ ì•ŠëŠ” í‚¤ì›Œë“œì˜ˆìš”.')

########################################################################################################################
# íŒŒì´ì°¨íŠ¸
with col6_2:
    st.markdown('**í‚¤ì›Œë“œ ê¸/ë¶€ì • ë¦¬ë·° ë¹„ìœ¨**')
    if len(í‚¤ì›Œë“œ) > 1:
        df_íŒŒì´ì°¨íŠ¸ = pd.DataFrame(ë§ˆìŠ¤í¬ëœë°ì´í„°í”„ë ˆì„['sentiment'][ë§ˆìŠ¤í¬ëœë°ì´í„°í”„ë ˆì„['review_sentence'].str.contains('|'.join(í‚¤ì›Œë“œ))].value_counts())
    if len(í‚¤ì›Œë“œ) == 1:
        df_íŒŒì´ì°¨íŠ¸ = pd.DataFrame(ë§ˆìŠ¤í¬ëœë°ì´í„°í”„ë ˆì„['sentiment'][ë§ˆìŠ¤í¬ëœë°ì´í„°í”„ë ˆì„['review_sentence'].str.contains(í‚¤ì›Œë“œ[0])].value_counts())
    pie_chart = go.Figure(data=[go.Pie(labels=list(df_íŒŒì´ì°¨íŠ¸.index), values=df_íŒŒì´ì°¨íŠ¸['count'])])
    st.plotly_chart(pie_chart, use_container_width=True)
########################################################################################################################
# 7. ë„½ì›¤ ë°ì´í„° í”„ë ˆì„
# with st.container():
#     col7_1, col7_2 = st.columns([3,1])

expander = st.expander('í‚¤ì›Œë“œê°€ í¬í•¨ëœ ë¦¬ë·°')
with expander:
    if len(í‚¤ì›Œë“œ) == 1:
        ë³´ì—¬ì¤„df = ë§ˆìŠ¤í¬ëœë°ì´í„°í”„ë ˆì„[ë§ˆìŠ¤í¬ëœë°ì´í„°í”„ë ˆì„['noun'].str.contains(í‚¤ì›Œë“œ[0])]
        st.dataframe(ë³´ì—¬ì¤„df[['name','sentiment','review_sentence', 'noun']])
        í‚¤ì›Œë“œ = [í‚¤ì›Œë“œ]
    elif len(í‚¤ì›Œë“œ) > 1:
        ë³´ì—¬ì¤„df = ë§ˆìŠ¤í¬ëœë°ì´í„°í”„ë ˆì„[ë§ˆìŠ¤í¬ëœë°ì´í„°í”„ë ˆì„['noun'].str.contains('|'.join(í‚¤ì›Œë“œ))]
        st.dataframe(ë³´ì—¬ì¤„df[['name','sentiment','review_sentence']], use_container_width=True)


# with col7_1:
#     if len(í‚¤ì›Œë“œ) == 1:
#         ë³´ì—¬ì¤„df = ë§ˆìŠ¤í¬ëœë°ì´í„°í”„ë ˆì„[ë§ˆìŠ¤í¬ëœë°ì´í„°í”„ë ˆì„['noun'].str.contains(í‚¤ì›Œë“œ[0])]
#         st.dataframe(ë³´ì—¬ì¤„df[['name','sentiment','review_sentence', 'noun', 'replace_slang_sentence']])
#         í‚¤ì›Œë“œ = [í‚¤ì›Œë“œ]
#     elif len(í‚¤ì›Œë“œ) > 1:
#         ë³´ì—¬ì¤„df = ë§ˆìŠ¤í¬ëœë°ì´í„°í”„ë ˆì„[ë§ˆìŠ¤í¬ëœë°ì´í„°í”„ë ˆì„['noun'].str.contains('|'.join(í‚¤ì›Œë“œ))]
#         st.dataframe(ë³´ì—¬ì¤„df[['name','sentiment','review_sentence']], use_container_width=True)

########################################################################################################################
import ast

fix_stop_words = [ 'í•©ë‹ˆë‹¤', 'í•˜ëŠ”', 'í• ', 'í•˜ê³ ', 'í•œë‹¤','í•˜ë‹¤','ë˜ë‹¤','ê°™ë‹¤','ìë‹¤','ë˜ë‹¤','ìˆë‹¤','ì¨ë‹¤','ì•Šë‹¤','í•´ë³´ë‹¤','ì£¼ë‹¤','ë˜ì–´ë‹¤', 
             'ê·¸ë¦¬ê³ ', 'ì…ë‹ˆë‹¤', 'ê·¸', 'ë“±', 'ì´ëŸ°', 'ë°','ì œ', 'ë”','ì–¸ëŠ˜','ê²°êµ­','ìƒê°','ì‹ë¬¼í‚¤',
             'ê°ì‚¬','ì§„ì§œ','ì™„ì „','ìš”ã…','ì‚¬ìš©','ì •ë„','ì—„ë§ˆ','ì•„ì´','ì›ë˜','ì‹ë¬¼','íí','í•˜í•˜','ì •ë§']

def to_list(text):
    return ast.literal_eval(text)

def lda_modeling(tokens, num_topics, passes=10):
    # word-document matrix
    dictionary = gensim.corpora.Dictionary(tokens)
    corpus = [dictionary.doc2bow(token) for token in tokens]

    # Train the LDA model
    model = gensim.models.ldamodel.LdaModel(corpus,
                                            num_topics=num_topics,
                                            id2word=dictionary, # ë‹¨ì–´ë§¤íŠ¸ë¦­ìŠ¤
                                            passes=passes, # í•™ìŠµë°˜ë³µíšŸìˆ˜
                                            random_state=100) 
    return model, corpus, dictionary

def print_topic_model(topics, rating, key):
    topic_values = []
    for topic in topics:
        topic_value = topic[1]
        topic_values.append(topic_value)
    topic_model = pd.DataFrame({"topic_num": list(range(1, len(topics) + 1)), "word_prop": topic_values})
    
    # í† ê¸€ ìƒì„±
    if st.checkbox('ì£¼ì œë³„ êµ¬ì„± ë‹¨ì–´ ë¹„ìœ¨ í™•ì¸', key=key):
    # í† ê¸€ì´ ì„ íƒë˜ì—ˆì„ ë•Œ ë°ì´í„°í”„ë ˆì„ ì¶œë ¥
        st.dataframe(topic_model, use_container_width=True)


# ì‹œê°í™”1. ê° ì£¼ì œì—ì„œ ìƒìœ„ Nê°œ í‚¤ì›Œë“œì˜ ì›Œë“œ í´ë¼ìš°ë“œ
def topic_wordcloud(model,num_topics):
    cand_mask = np.array(Image.open('/app/busypeople-streamlit/data/circle.png'))
    cloud = WordCloud(background_color='white',
                      font_path = "/app/busypeople-streamlit/font/NanumBarunGothic.ttf",
                      width=500,
                      height=500,
                      max_words=7,
                      colormap='tab10',
                      prefer_horizontal=1.0,
                      mask=cand_mask)
    
    topics = model.show_topics(formatted=False)

    # ëª¨ë¸ë§ˆë‹¤ í† í”½ê°œìˆ˜ê°€ ë‹¬ë¼ì„œ rows, colsì´ í† í”½ì˜ ê°œìˆ˜ë§ˆë‹¤ ë°”ë€œì£¼ê¸°
    fig, axes = plt.subplots(1, num_topics, figsize=(12,8), sharex=True, sharey=True)

    for i, ax in enumerate(axes.flatten()):
        fig.add_subplot(ax)
        topic_words = dict(topics[i][1])
        cloud.generate_from_frequencies(topic_words, max_font_size=300)
        plt.gca().imshow(cloud)
        plt.gca().set_title('Topic ' + str(i), fontdict=dict(size=16))
        plt.gca().axis('off')

    plt.subplots_adjust(wspace=0, hspace=0)
    plt.axis('off')
    plt.margins(x=0, y=0)
    plt.tight_layout()
    st.pyplot(fig, use_container_width=True)

# ëª…ì‚¬ê¸°ì¤€ í† í”½ë¶„ì„(7ê°œì”© ë‚˜ì˜¤ê²Œ í•œê±´ ì´ì „ ì—°êµ¬ìë£Œë“¤ ì°¸ê³ )
def n_get_topic_model(data, topic_number, passes=10, num_words=7, key=None):
    df = pd.read_csv(data)

    # ë¶ˆìš©ì–´ ë¦¬ìŠ¤íŠ¸
    stopwords = stop_words

    # ë¬¸ì¥ ë¦¬ìŠ¤íŠ¸ ìƒì„±
    reviews = []
    for i in range(len(df)):
        reviews.append(to_list(df['noun'][i]))

    # í…ìŠ¤íŠ¸ ë°ì´í„° ì „ì²˜ë¦¬
    # ë¶ˆìš©ì–´ ì œê±°, ë‹¨ì–´ ì¸ì½”ë”© ë° ë¹ˆë„ìˆ˜ ê³„ì‚°
    tokens = []
    for review in reviews:
        token_review = [w for w in review if len(w) > 1 and w not in stopwords]
        tokens.append(token_review)

    # # LDA ëª¨ë¸ë§
    model, corpus, dictionary = lda_modeling(tokens, num_topics = topic_number, passes=passes)

    rating = 'pos' 
    topics = model.print_topics(num_words=num_words)
    print_topic_model(topics, rating, key)

    # í† í”½ë³„ ì›Œë“œí´ë¼ìš°ë“œ ì‹œê°í™”
    topic_wordcloud(model, num_topics=topic_number)

# ëª…ì‚¬+ë™ì‚¬+í˜•ìš©ì‚¬ ê¸°ì¤€ í† í”½ë¶„ì„
def nv_get_topic_model(data, topic_number, passes=10, num_words=7, key=None):
    df = pd.read_csv(data)

    # ë¶ˆìš©ì–´ ë¦¬ìŠ¤íŠ¸
    stopwords = stop_words

    # ë¬¸ì¥ ë¦¬ìŠ¤íŠ¸ ìƒì„±
    reviews = []
    for i in range(len(df)):
        reviews.append(to_list(df['n_v_ad'][i]))

    # í…ìŠ¤íŠ¸ ë°ì´í„° ì „ì²˜ë¦¬
    # ë¶ˆìš©ì–´ ì œê±°, ë‹¨ì–´ ì¸ì½”ë”© ë° ë¹ˆë„ìˆ˜ ê³„ì‚°
    tokens = []
    for review in reviews:
        token_review = [w for w in review if len(w) > 1 and w not in stopwords]
        tokens.append(token_review)

    # # LDA ëª¨ë¸ë§
    model, corpus, dictionary = lda_modeling(tokens, num_topics=topic_number, passes=passes)

    rating = 'pos' 
    topics = model.print_topics(num_words=num_words)
    print_topic_model(topics, rating, key)

    # í† í”½ë³„ ì›Œë“œí´ë¼ìš°ë“œ ì‹œê°í™”
    topic_wordcloud(model, num_topics=topic_number)

st.write("")
st.write("")
st.write("")
########################ì—¬ê¸°ì„œë¶€í„° streamlit êµ¬í˜„ #########################
st.subheader('**ğŸ”SWOT ë¶„ì„**')
tab1, tab2, tab3, tab4 = st.tabs(["**Strength(ê°•ì )**", "**Weakness(ì•½ì )**", "**Opportunity(ê¸°íšŒ)**", "**Threat(ìœ„í˜‘)**"])

with tab1:
    col1_, col2_ = st.columns(2)    

    with col1_:
        n_v_type = st.selectbox('ë°ì´í„° ìœ í˜•',['ëª…ì‚¬', 'ëª…ì‚¬+ë™ì‚¬+í˜•ìš©ì‚¬'], key='selectbox1')
    with col2_:
        input_str = st.text_input('ì œê±°í•  í‚¤ì›Œë“œ :', key='stopwords_input1')
        stop_words = fix_stop_words.copy()
        stopwords = stop_words.extend([x.strip() for x in input_str.split(',')])

    st.write('ìì‚¬ ê¸ì •ë¦¬ë·°ë“¤ì˜ ì£¼ì œë³„ í‚¤ì›Œë“œë¥¼ ë¶„ì„í•œ ê²°ê³¼ì…ë‹ˆë‹¤. :sunglasses:')

    file_path = '/app/busypeople-streamlit/data/ìì‚¬ê¸ì •(10ì°¨).csv'

    if n_v_type =='ëª…ì‚¬':
        n_get_topic_model(file_path,9 , key='ì¤€íƒ±ì´1')
    else:
        nv_get_topic_model(file_path,10, key='ì¤€íƒ±ì´2')

with tab2:
    col1_2_, col2_2_ = st.columns(2)    

    with col1_2_:
        n_v_type = st.selectbox('ë°ì´í„° ìœ í˜•',['ëª…ì‚¬', 'ëª…ì‚¬+ë™ì‚¬+í˜•ìš©ì‚¬'], key='selectbox2')
    with col2_2_:
        input_str = st.text_input('ì œê±°í•  í‚¤ì›Œë“œ :', key='stopwords_input2')
        stop_words = fix_stop_words.copy()
        stopwords = stop_words.extend([x.strip() for x in input_str.split(',')])

    st.write('ìì‚¬ ë¶€ì •ë¦¬ë·°ë“¤ì˜ ì£¼ì œë³„ í‚¤ì›Œë“œë¥¼ ë¶„ì„í•œ ê²°ê³¼ì…ë‹ˆë‹¤. :sweat:')

    file_path = '/app/busypeople-streamlit/data/ìì‚¬ë¶€ì •(10ì°¨).csv'

    if n_v_type =='ëª…ì‚¬':
        n_get_topic_model(file_path,4, key='ì¤€íƒ±ì´3')
    else:
        nv_get_topic_model(file_path,8, key='ì¤€íƒ±ì´4')

# ì¶”ê°€ìˆ˜ì • í•„ìš”############################################################################
with tab3:
    col1_3_, col2_3_, col3_3_ = st.columns(3)    

    with col1_3_:
        n_v_type = st.selectbox('ë°ì´í„° ìœ í˜•',['ëª…ì‚¬', 'ëª…ì‚¬+ë™ì‚¬+í˜•ìš©ì‚¬'], key='selectbox3')
    with col2_3_:
        d_type = st.selectbox('ì œí’ˆ',['ê²½ìŸì‚¬-ì‹ë¬¼ì˜ì–‘ì œ', 'ê²½ìŸì‚¬-ë¿Œë¦¬ì˜ì–‘ì œ', 'ê²½ìŸì‚¬-ì‚´ì¶©ì œ',
                                     'ê²½ìŸì‚¬-ì‹ë¬¼ë“±', 'ê²½ìŸì‚¬ ì „ì²´'], key='selectbox3_1_')
    with col3_3_:
        input_str = st.text_input('ì œê±°í•  í‚¤ì›Œë“œ :', key='stopwords_input3')
        stop_words = fix_stop_words.copy()
        stopwords = stop_words.extend([x.strip() for x in input_str.split(',')])
    
    st.write('ê²½ìŸì‚¬ ë¶€ì •ë¦¬ë·°ë“¤ì˜ ì£¼ì œë³„ í‚¤ì›Œë“œë¥¼ ë¶„ì„í•œ ê²°ê³¼ì…ë‹ˆë‹¤. :wink:')

    if n_v_type =='ëª…ì‚¬' and d_type == 'ê²½ìŸì‚¬ ì „ì²´' :
        file_path = '/app/busypeople-streamlit/data/ê²½ìŸì‚¬ë¶€ì •(10ì°¨).csv'
        n_get_topic_model(file_path,10, key='ì¤€íƒ±ì´5')
    elif n_v_type =='ëª…ì‚¬+ë™ì‚¬+í˜•ìš©ì‚¬' and d_type == 'ê²½ìŸì‚¬ ì „ì²´' :
        file_path = '/app/busypeople-streamlit/data/ê²½ìŸì‚¬ë¶€ì •(10ì°¨).csv'
        nv_get_topic_model(file_path,8, key='ì¤€íƒ±ì´6')

    elif n_v_type =='ëª…ì‚¬' and d_type == 'ê²½ìŸì‚¬-ì‹ë¬¼ì˜ì–‘ì œ' :
        file_path = '/app/busypeople-streamlit/data/ê²½ìŸì‚¬(ì‹ë¬¼ì˜ì–‘ì œ)ë¶€ì •(10ì°¨).csv'
        n_get_topic_model(file_path,5, key='ì¤€íƒ±ì´7')
    elif n_v_type =='ëª…ì‚¬+ë™ì‚¬+í˜•ìš©ì‚¬' and d_type == 'ê²½ìŸì‚¬-ì‹ë¬¼ì˜ì–‘ì œ' :
        file_path = '/app/busypeople-streamlit/data/ê²½ìŸì‚¬(ì‹ë¬¼ì˜ì–‘ì œ)ë¶€ì •(10ì°¨).csv'
        nv_get_topic_model(file_path,6, key='ì¤€íƒ±ì´8')
    
    elif n_v_type =='ëª…ì‚¬' and d_type == 'ê²½ìŸì‚¬-ë¿Œë¦¬ì˜ì–‘ì œ' :
        file_path = '/app/busypeople-streamlit/data/ê²½ìŸì‚¬(ë¿Œë¦¬ì˜ì–‘ì œ)ë¶€ì •(10ì°¨).csv'
        n_get_topic_model(file_path,5, key='ì¤€íƒ±ì´9')
    elif n_v_type =='ëª…ì‚¬+ë™ì‚¬+í˜•ìš©ì‚¬' and d_type == 'ê²½ìŸì‚¬-ë¿Œë¦¬ì˜ì–‘ì œ' :
        file_path = '/app/busypeople-streamlit/data/ê²½ìŸì‚¬(ë¿Œë¦¬ì˜ì–‘ì œ)ë¶€ì •(10ì°¨).csv'
        nv_get_topic_model(file_path,8, key='ì¤€íƒ±ì´10')
    
    elif n_v_type =='ëª…ì‚¬' and d_type == 'ê²½ìŸì‚¬-ì‚´ì¶©ì œ' :
        file_path = '/app/busypeople-streamlit/data/ê²½ìŸì‚¬(ì‚´ì¶©ì œ)ë¶€ì •(10ì°¨).csv'
        n_get_topic_model(file_path,4, key='ì¤€íƒ±ì´11')
    elif n_v_type =='ëª…ì‚¬+ë™ì‚¬+í˜•ìš©ì‚¬' and d_type == 'ê²½ìŸì‚¬-ì‚´ì¶©ì œ' :
        file_path = '/app/busypeople-streamlit/data/ê²½ìŸì‚¬(ì‚´ì¶©ì œ)ë¶€ì •(10ì°¨).csv'
        nv_get_topic_model(file_path,10, key='ì¤€íƒ±ì´12')
    
    elif n_v_type =='ëª…ì‚¬' and d_type == 'ê²½ìŸì‚¬-ì‹ë¬¼ë“±' :
        file_path = '/app/busypeople-streamlit/data/ê²½ìŸì‚¬(ì‹ë¬¼ë“±)ë¶€ì •(10ì°¨).csv'
        n_get_topic_model(file_path,9, key='ì¤€íƒ±ì´13')
    elif n_v_type =='ëª…ì‚¬+ë™ì‚¬+í˜•ìš©ì‚¬' and d_type == 'ê²½ìŸì‚¬-ì‹ë¬¼ë“±' :
        file_path = '/app/busypeople-streamlit/data/ê²½ìŸì‚¬(ì‹ë¬¼ë“±)ë¶€ì •(10ì°¨).csv'
        nv_get_topic_model(file_path,10, key='ì¤€íƒ±ì´14')
    

with tab4:
    col1_4_, col2_4_, col3_4_ = st.columns(3)     

    with col1_4_:
        n_v_type = st.selectbox('ë°ì´í„° ìœ í˜•',['ëª…ì‚¬', 'ëª…ì‚¬+ë™ì‚¬+í˜•ìš©ì‚¬'], key='selectbox4')
    
    with col2_4_:
        d_type = st.selectbox('ì œí’ˆ',['ê²½ìŸì‚¬-ì‹ë¬¼ì˜ì–‘ì œ', 'ê²½ìŸì‚¬-ë¿Œë¦¬ì˜ì–‘ì œ', 'ê²½ìŸì‚¬-ì‚´ì¶©ì œ',
                                     'ê²½ìŸì‚¬-ì‹ë¬¼ë“±', 'ê²½ìŸì‚¬ ì „ì²´'], key='selectbox4_1_')
    with col3_4_:
        input_str = st.text_input('ì œê±°í•  í‚¤ì›Œë“œ :', key='stopwords_input4')
        stop_words = fix_stop_words.copy()
        stopwords = stop_words.extend([x.strip() for x in input_str.split(',')])

    st.write('ê²½ìŸì‚¬ ê¸ì •ë¦¬ë·°ë“¤ì˜ ì£¼ì œë³„ í‚¤ì›Œë“œë¥¼ ë¶„ì„í•œ ê²°ê³¼ì…ë‹ˆë‹¤. :confounded:')

    if n_v_type =='ëª…ì‚¬' and d_type == 'ê²½ìŸì‚¬ ì „ì²´' :
        file_path = '/app/busypeople-streamlit/data/ê²½ìŸì‚¬ê¸ì •(10ì°¨).csv'
        n_get_topic_model(file_path,10, key='ì¤€íƒ±ì´15')
    elif n_v_type =='ëª…ì‚¬+ë™ì‚¬+í˜•ìš©ì‚¬' and d_type == 'ê²½ìŸì‚¬ ì „ì²´' :
        file_path = '/app/busypeople-streamlit/data/ê²½ìŸì‚¬ê¸ì •(10ì°¨).csv'
        nv_get_topic_model(file_path,10, key='ì¤€íƒ±ì´16')

    elif n_v_type =='ëª…ì‚¬' and d_type == 'ê²½ìŸì‚¬-ì‹ë¬¼ì˜ì–‘ì œ' :
        file_path = '/app/busypeople-streamlit/data/ê²½ìŸì‚¬(ì‹ë¬¼ì˜ì–‘ì œ)ê¸ì •(10ì°¨).csv'
        n_get_topic_model(file_path,10, key='ì¤€íƒ±ì´17')
    elif n_v_type =='ëª…ì‚¬+ë™ì‚¬+í˜•ìš©ì‚¬' and d_type == 'ê²½ìŸì‚¬-ì‹ë¬¼ì˜ì–‘ì œ' :
        file_path = '/app/busypeople-streamlit/data/ê²½ìŸì‚¬(ì‹ë¬¼ì˜ì–‘ì œ)ê¸ì •(10ì°¨).csv'
        nv_get_topic_model(file_path,8, key='ì¤€íƒ±ì´18')
    
    elif n_v_type =='ëª…ì‚¬' and d_type == 'ê²½ìŸì‚¬-ë¿Œë¦¬ì˜ì–‘ì œ' :
        file_path = '/app/busypeople-streamlit/data/ê²½ìŸì‚¬(ë¿Œë¦¬ì˜ì–‘ì œ)ê¸ì •(10ì°¨).csv'
        n_get_topic_model(file_path,10, key='ì¤€íƒ±ì´19')
    elif n_v_type =='ëª…ì‚¬+ë™ì‚¬+í˜•ìš©ì‚¬' and d_type == 'ê²½ìŸì‚¬-ë¿Œë¦¬ì˜ì–‘ì œ' :
        file_path = '/app/busypeople-streamlit/data/ê²½ìŸì‚¬(ë¿Œë¦¬ì˜ì–‘ì œ)ê¸ì •(10ì°¨).csv'
        nv_get_topic_model(file_path,10, key='ì¤€íƒ±ì´20')
    
    elif n_v_type =='ëª…ì‚¬' and d_type == 'ê²½ìŸì‚¬-ì‚´ì¶©ì œ' :
        file_path = '/app/busypeople-streamlit/data/ê²½ìŸì‚¬(ì‚´ì¶©ì œ)ê¸ì •(10ì°¨).csv'
        n_get_topic_model(file_path,10, key='ì¤€íƒ±ì´21')
    elif n_v_type =='ëª…ì‚¬+ë™ì‚¬+í˜•ìš©ì‚¬' and d_type == 'ê²½ìŸì‚¬-ì‚´ì¶©ì œ' :
        file_path = '/app/busypeople-streamlit/data/ê²½ìŸì‚¬(ì‚´ì¶©ì œ)ê¸ì •(10ì°¨).csv'
        nv_get_topic_model(file_path,9, key='ì¤€íƒ±ì´22')
    
    elif n_v_type =='ëª…ì‚¬' and d_type == 'ê²½ìŸì‚¬-ì‹ë¬¼ë“±' :
        file_path = '/app/busypeople-streamlit/data/ê²½ìŸì‚¬(ì‹ë¬¼ë“±)ê¸ì •(10ì°¨).csv'
        n_get_topic_model(file_path,10, key='ì¤€íƒ±ì´23')
    elif n_v_type =='ëª…ì‚¬+ë™ì‚¬+í˜•ìš©ì‚¬' and d_type == 'ê²½ìŸì‚¬-ì‹ë¬¼ë“±' :
        file_path = '/app/busypeople-streamlit/data/ê²½ìŸì‚¬(ì‹ë¬¼ë“±)ê¸ì •(10ì°¨).csv'
        nv_get_topic_model(file_path,9, key='ì¤€íƒ±ì´24')


########################################################################################################################
st.write("")
st.write("")
st.write("")
########################Tableau êµ¬í˜„ #########################
st.subheader('**ğŸ”ìì‚¬/ê²½ìŸì‚¬ ë¦¬ë·° ë¶„ë¥˜ ë¶„ì„**')
tab1, tab2= st.tabs(["**ìì‚¬**", "**ê²½ìŸì‚¬**"])

with tab1:
    st.write('ì œí’ˆ/ë°°ì†¡/ì‚¬ìš©ë²•ìœ¼ë¡œ ìì‚¬ì˜ ë¦¬ë·°ë¥¼ ë¶„ë¥˜í•œ ê²°ê³¼ì…ë‹ˆë‹¤. ğŸ˜Š')
    with st.container():
        url = "https://public.tableau.com/views/1_16835240938720/1_1?:language=ko-KR&:showVizHome=no&:embed=true"
        html = f'''
            <iframe src={url} style="width:100%; height:70vh"></iframe>
        '''
        st.markdown(html, unsafe_allow_html=True)

with tab2:
    st.write('ì œí’ˆ/ë°°ì†¡/ì‚¬ìš©ë²•ìœ¼ë¡œ ê²½ìŸì‚¬ì˜ ë¦¬ë·°ë¥¼ ë¶„ë¥˜í•œ ê²°ê³¼ì…ë‹ˆë‹¤. ğŸ˜Š')
    with st.container():
        url = "https://public.tableau.com/views/_16835258920980/1_1?:language=ko-KR&:showVizHome=no&:embed=true"
        html = f'''
            <iframe src={url} style="width:100%; height:70vh"></iframe>
        '''
        st.markdown(html, unsafe_allow_html=True)
########################################################################################################################
########################################################################################################################
########################################################################################################################
########################################################################################################################